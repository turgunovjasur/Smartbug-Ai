# test_chunking_system.py
"""
Smart Chunking System Test Script

Bu script chunking quality va embedding accuracy'ni test qiladi
"""

import sys
import os

from utils.chunking_helper import ChunkingHelper

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from metadata_helper import MetadataHelper


def print_section(title):
    """Print formatted section header"""
    print("\n" + "=" * 80)
    print(f"üîç {title}")
    print("=" * 80 + "\n")


def test_multilingual_detection():
    """Test language detection"""
    print_section("TEST 1: Multilingual Detection")

    chunker = ChunkingHelper()

    test_cases = [
        ("Login page error occurred", "en"),
        ("–û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –≤—Ö–æ–¥–∞", "ru"),
        ("Login sahifasida xatolik", "uz"),
        ("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –º–æ–∂–µ—Ç –≤–æ–π—Ç–∏ –≤ —Å–∏—Å—Ç–µ–º—É", "ru"),
        ("User can't login to system", "en"),
        ("Foydalanuvchi tizimga kira olmayapti", "uz"),
        ("This is mixed –í–∞ —ç—Ç–æ —Ä—É—Å—Å–∫–∏–π", "mixed")
    ]

    results = []
    for text, expected_lang in test_cases:
        detected = chunker._detect_primary_language(text)
        status = "‚úÖ" if detected == expected_lang else "‚ùå"
        results.append((status, text[:50], expected_lang, detected))

    # Display results
    print(f"{'Status':<8} {'Text':<55} {'Expected':<10} {'Detected':<10}")
    print("-" * 85)
    for status, text, expected, detected in results:
        print(f"{status:<8} {text:<55} {expected:<10} {detected:<10}")

    success_rate = sum(1 for r in results if r[0] == "‚úÖ") / len(results) * 100
    print(f"\nüìä Success Rate: {success_rate:.1f}%")


def test_root_cause_detection():
    """Test root cause extraction"""
    print_section("TEST 2: Root Cause Detection")

    chunker = ChunkingHelper()

    test_cases = [
        {
            'text': """
            Bug Report: Login fails

            Root cause: The authentication service was not properly handling null 
            tokens. Due to a race condition in the token validation logic, some 
            requests were processed without valid tokens.

            Steps: User clicks login -> Error 500
            """,
            'should_detect': True,
            'language': 'en'
        },
        {
            'text': """
            –ë–ê–ì: –û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏

            –ü—Ä–∏—á–∏–Ω–∞: –°–µ—Ä–≤–∏—Å –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª –ø—É—Å—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã.
            –ò–∑-–∑–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≥–æ–Ω–∫–∏ –≤ –ª–æ–≥–∏–∫–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–ø—Ä–æ—Å—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∏—Å—å
            –±–µ–∑ –≤–∞–ª–∏–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤.

            –ü—Ä–æ–±–ª–µ–º–∞ –≤ —Ç–æ–º —á—Ç–æ —Ç–æ–∫–µ–Ω –Ω–µ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è.
            """,
            'should_detect': True,
            'language': 'ru'
        },
        {
            'text': """
            BUG: Login xatoligi

            Sabab: Authentication service null tokenlarni to'g'ri handle qilmayapti.
            Token validation logikasida race condition sababli ba'zi requestlar
            valid token'siz process bo'lmoqda.

            Muammo shundaki token tekshirilmayapti.
            """,
            'should_detect': True,
            'language': 'uz'
        },
        {
            'text': """
            Simple description without root cause.
            User can't login. Please fix it.
            """,
            'should_detect': False,
            'language': 'en'
        }
    ]

    print(f"{'Test':<6} {'Language':<10} {'Should Detect':<15} {'Detected':<10} {'Status':<8}")
    print("-" * 60)

    results = []
    for i, test in enumerate(test_cases, 1):
        root_cause = chunker._extract_root_cause(test['text'])
        detected = len(root_cause) > 0
        should_detect = test['should_detect']
        status = "‚úÖ" if detected == should_detect else "‚ùå"

        print(f"{i:<6} {test['language']:<10} {str(should_detect):<15} "
              f"{str(detected):<10} {status:<8}")

        if detected:
            print(f"   ‚Üí Extracted: {root_cause[:100]}...")

        results.append(status == "‚úÖ")

    success_rate = sum(results) / len(results) * 100
    print(f"\nüìä Success Rate: {success_rate:.1f}%")


def test_solution_extraction():
    """Test solution extraction"""
    print_section("TEST 3: Solution Extraction")

    chunker = ChunkingHelper()

    test_cases = [
        {
            'text': """
            Solution: Added null check in token validation. 
            Fixed by implementing proper token verification before processing requests.
            Changed the authentication flow to validate tokens first.
            """,
            'should_detect': True,
            'language': 'en'
        },
        {
            'text': """
            –†–µ—à–µ–Ω–∏–µ: –î–æ–±–∞–≤–ª–µ–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ null –≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–∞.
            –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ –ø—É—Ç–µ–º —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–∞.
            –ò–∑–º–µ–Ω–µ–Ω flow –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤ —Å–Ω–∞—á–∞–ª–∞.
            """,
            'should_detect': True,
            'language': 'ru'
        },
        {
            'text': """
            Yechim: Token validation'ga null check qo'shildi.
            Token'ni tekshirish uchun to'g'ri verification qo'shish orqali tuzatildi.
            Authentication flow o'zgartirildi - avval token tekshiriladi.
            """,
            'should_detect': True,
            'language': 'uz'
        }
    ]

    print(f"{'Test':<6} {'Language':<10} {'Detected':<10} {'Status':<8}")
    print("-" * 40)

    results = []
    for i, test in enumerate(test_cases, 1):
        solution = chunker._extract_solution(test['text'])
        detected = len(solution) > 0
        status = "‚úÖ" if detected else "‚ùå"

        print(f"{i:<6} {test['language']:<10} {str(detected):<10} {status:<8}")

        if detected:
            print(f"   ‚Üí Extracted: {solution[:100]}...")

        results.append(status == "‚úÖ")

    success_rate = sum(results) / len(results) * 100
    print(f"\nüìä Success Rate: {success_rate:.1f}%")


def test_chunk_creation():
    """Test full chunk creation"""
    print_section("TEST 4: Full Chunk Creation")

    chunker = ChunkingHelper(max_chunk_length=800)

    # Test issue data
    issue_data = {
        'key': 'TEST-123',
        'summary': 'Login authentication fails for users',
        'description': """
        Users are unable to login to the system.

        Root cause: The authentication service was not properly handling null 
        tokens. Due to a race condition in the token validation logic, some 
        requests were processed without valid tokens.

        Solution: Added null check in token validation and fixed the race 
        condition by implementing proper synchronization.
        """,
        'comments': """
        [2025-01-01] Developer: Investigating the issue.
        [2025-01-02] QA: Still failing in production.
        [2025-01-03] Developer: Fixed by adding token validation.
        """,
        'return_reasons': """
        Return #1 [2025-01-02]: TESTING ‚Üí RETURN TEST (by QA Team)
        Reason: Authentication still fails
        """,
        'status_history': """
        2025-01-01 10:00: None ‚Üí IN PROGRESS
        2025-01-01 15:00: IN PROGRESS ‚Üí TESTING
        2025-01-02 09:00: TESTING ‚Üí RETURN TEST
        2025-01-03 14:00: RETURN TEST ‚Üí TESTING
        2025-01-03 17:00: TESTING ‚Üí CLOSED
        """,
        'type': 'Bug',
        'priority': 'High',
        'assignee': 'John Doe',
        'reporter': 'QA Team',
        'components': 'Authentication, Security',
        'labels': 'production, critical',
        'story_points': '5',
        'return_count': 1,
        'pr_status': 'MERGED'
    }

    # Create chunks
    chunks = chunker.create_chunks(issue_data)

    print(f"Total Chunks: {len(chunks)}\n")

    # Display chunks
    chunk_stats = {}
    for i, chunk in enumerate(chunks, 1):
        chunk_type = chunk['type']
        chunk_stats[chunk_type] = chunk_stats.get(chunk_type, 0) + 1

        print(f"Chunk {i}: {chunk_type.upper()}")
        print(f"   Weight: {chunk['weight']}")
        print(f"   Language: {chunk['language']}")
        print(f"   Length: {len(chunk['text'])} chars")
        print(f"   Text: {chunk['text'][:150]}...")
        print()

    # Statistics
    print("üìä Chunk Type Distribution:")
    for chunk_type, count in sorted(chunk_stats.items(), key=lambda x: x[1], reverse=True):
        print(f"   ‚Ä¢ {chunk_type}: {count}")

    # Check detection
    has_root_cause = any(c['type'] == 'root_cause' for c in chunks)
    has_solution = any(c['type'] == 'solution' for c in chunks)

    print(f"\nüéØ Detection Results:")
    print(f"   ‚Ä¢ Root Cause: {'‚úÖ Detected' if has_root_cause else '‚ùå Not detected'}")
    print(f"   ‚Ä¢ Solution: {'‚úÖ Detected' if has_solution else '‚ùå Not detected'}")

    # Weighted average simulation
    total_weight = sum(c['weight'] for c in chunks)
    print(f"\n‚öñÔ∏è  Total Weight: {total_weight:.1f}")


def test_metadata_extraction():
    """Test metadata extraction"""
    print_section("TEST 5: Metadata Extraction")

    test_issue = {
        'key': 'TEST-456',
        'type': 'Bug',
        'status': 'Closed',
        'sprint_id': '2842',
        'assignee': 'Developer Name',
        'reporter': 'QA Team',
        'priority': 'High',
        'story_points': 5,
        'created_date': '2025-01-01 10:00:00',
        'resolved_date': '2025-01-05 17:00:00',
        'comments': 'Some comments here',
        'return_count': 2,
        'labels': 'production, critical',
        'components': 'Authentication, API',
        'pr_status': 'MERGED',
        'pr_count': 1,
        'testing_time': '2.5h',
        'linked_issues': 'TEST-123, TEST-789'
    }

    # Extract metadata
    search_meta = MetadataHelper.extract_search_metadata(test_issue)
    display_info = MetadataHelper.extract_display_info(test_issue)

    print("üîç Search Metadata (for VectorDB filters):")
    for key, value in sorted(search_meta.items()):
        print(f"   ‚Ä¢ {key}: {value}")

    print("\nüì∫ Display Info (for UI):")
    for key, value in sorted(display_info.items()):
        print(f"   ‚Ä¢ {key}: {value}")

    # Test filter creation
    print("\nüéØ Sample Filters:")

    filter1 = MetadataHelper.create_search_filters(
        types=['Bug'],
        statuses=['Closed', 'Done']
    )
    print(f"\n1. Bug search filter:")
    print(f"   {filter1}")

    filter2 = MetadataHelper.create_search_filters(
        types=['Bug'],
        min_return_count=1,
        has_pr=True
    )
    print(f"\n2. Bugs with returns and PR:")
    print(f"   {filter2}")


def test_full_pipeline():
    """Test kelajakda - full embedding pipeline"""
    print_section("TEST 6: Full Pipeline (Placeholder)")

    print("‚ö†Ô∏è  Bu test embedding_helper va vectordb_helper ni talab qiladi.")
    print("   Faqat chunking va metadata testlari tugadi.\n")
    print("   Full pipeline test uchun 2_load_sprints_v2.py ni ishlatish kerak.")


def main():
    """Run all tests"""
    print("\n" + "=" * 80)
    print("üß™ SMART CHUNKING SYSTEM - TEST SUITE")
    print("=" * 80)

    try:
        test_multilingual_detection()
        test_root_cause_detection()
        test_solution_extraction()
        test_chunk_creation()
        test_metadata_extraction()
        test_full_pipeline()

        print("\n" + "=" * 80)
        print("‚úÖ ALL TESTS COMPLETED")
        print("=" * 80 + "\n")

    except Exception as e:
        print(f"\n‚ùå TEST FAILED: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()